# üî¨ Procedimiento de Validaci√≥n de Modelos de IA

## üìã Resumen Ejecutivo

Este procedimiento establece el marco metodol√≥gico para la validaci√≥n de todos los modelos de Inteligencia Artificial antes de su despliegue en producci√≥n, garantizando su confiabilidad, equidad y alineaci√≥n con mejores pr√°cticas de la industria y est√°ndares regulatorios aplicables.

### Informaci√≥n Clave
- **Aplicaci√≥n**: Todos los modelos de IA/ML en el banco
- **Frecuencia**: Pre-producci√≥n + validaci√≥n peri√≥dica
- **Responsable**: Unidad de Validaci√≥n de Modelos (MRM)
- **Actualizaci√≥n**: Semestral o por cambio regulatorio

## üéØ Objetivos y Alcance

### Objetivos Principales
1. **Verificar** la precisi√≥n y robustez de los modelos
2. **Evaluar** riesgos de sesgo y discriminaci√≥n
3. **Asegurar** cumplimiento regulatorio
4. **Documentar** hallazgos y recomendaciones
5. **Certificar** aptitud para producci√≥n

### Alcance
- ‚úÖ Modelos de Machine Learning supervisados y no supervisados
- ‚úÖ Modelos de Deep Learning y redes neuronales
- ‚úÖ Modelos de IA Generativa y LLMs
- ‚úÖ Sistemas de reglas expertas con componentes de IA
- ‚ùå An√°lisis estad√≠sticos tradicionales sin ML

## üèóÔ∏è Framework de Validaci√≥n

### Estructura de Tres L√≠neas de Defensa

```mermaid
graph TB
    subgraph "1ra L√≠nea - Desarrollo"
        A[Product Owner] --> B[Data Scientists]
        B --> C[Auto-validaci√≥n]
    end
    
    subgraph "2da L√≠nea - Validaci√≥n"
        D[Model Risk Management] --> E[Validaci√≥n Independiente]
        E --> F[Certificaci√≥n]
    end
    
    subgraph "3ra L√≠nea - Auditor√≠a"
        G[Auditor√≠a Interna] --> H[Revisi√≥n Peri√≥dica]
        H --> I[Informe al Consejo]
    end
    
    C --> D
    F --> G
    
    style A fill:#667eea
    style D fill:#764ba2
    style G fill:#f093fb
```

## üìä Categorizaci√≥n de Modelos por Riesgo

> **Nota**: La clasificaci√≥n de modelos por Tiers se define en la [Pol√≠tica de Uso Responsable de IA](politica-uso-responsable-ia.md#clasificaci√≥n-de-riesgo-de-modelos). Este procedimiento detalla los requisitos de validaci√≥n para cada nivel.

### Matriz de Validaci√≥n por Tier

| Tier | Validaci√≥n Requerida | Timeline | Frecuencia | Validador |
|------|---------------------|----------|------------|-----------|
| **Tier 1 - Cr√≠tico** | Completa (8 fases) + Externa | 8 semanas | Semestral | Interno + Externo |
| **Tier 2 - Alto** | Completa (8 fases) | 6 semanas | Anual | Interno Senior |
| **Tier 3 - Moderado** | Est√°ndar (5 fases condensadas) | 3-4 semanas | Anual | Interno |
| **Tier 4 - Bajo** | Simplificada (fast-track) | 1-2 semanas | Bienal | Auto-validaci√≥n + Revisi√≥n |

### Rutas de Validaci√≥n Diferenciadas

#### üöÄ Fast-Track (Tier 4)
- Revisi√≥n documental + Replicaci√≥n b√°sica
- Pruebas automatizadas de performance
- Checklist de compliance simplificado
- Timeline: 5-10 d√≠as h√°biles

#### üìã Est√°ndar (Tier 3)
- Fases 1-3 completas
- Fases 4-5 combinadas (Fairness + Explicabilidad)
- Fase 6 simplificada (solo critical paths)
- Timeline: 15-20 d√≠as h√°biles

#### üî¨ Completa (Tier 1-2)
- Todas las fases detalladas
- Modelo challenger obligatorio
- Validaci√≥n externa para Tier 1
- Timeline: 30-40 d√≠as h√°biles

## üîÑ Proceso de Validaci√≥n Detallado

### Fase 1: Pre-Validaci√≥n (Semana 1)

#### 1.1 Recepci√≥n de Documentaci√≥n
- [ ] Model Development Document (MDD)
- [ ] Technical Design Document (TDD)
- [ ] Dataset Documentation
- [ ] C√≥digo fuente y notebooks
- [ ] Resultados de pruebas internas

#### 1.2 Evaluaci√≥n de Completitud
```python
# Checklist automatizado
validation_checklist = {
    "documentation": ["MDD", "TDD", "Data_Dict"],
    "code": ["training_script", "inference_script", "tests"],
    "data": ["train_set", "test_set", "validation_set"],
    "metrics": ["performance", "fairness", "stability"]
}
```

#### 1.3 Asignaci√≥n de Validador
- Validador sin conflicto de inter√©s
- Expertise en el dominio del modelo
- Certificaci√≥n en validaci√≥n de modelos

### Fase 2: Validaci√≥n Conceptual (Semana 2)

#### 2.1 Revisi√≥n de Dise√±o
| Aspecto | Criterios de Evaluaci√≥n | Evidencia Requerida |
|---------|------------------------|-------------------|
| **Idoneidad** | Metodolog√≠a apropiada para el problema | Literatura, benchmarks |
| **Supuestos** | Validez de assumptions del modelo | An√°lisis estad√≠stico |
| **Limitaciones** | Documentaci√≥n clara de restricciones | Casos de falla conocidos |
| **Alternativas** | Justificaci√≥n vs otros enfoques | Comparativa de m√©todos |

#### 2.2 Modelo Challenger (Obligatorio para Tier 1-2)

Para modelos de alto riesgo, se requiere comparaci√≥n con un modelo m√°s simple:

```python
# Validaci√≥n Champion vs Challenger
challenger_requirements = {
    "model_type": "interpretable",  # e.g., logistic regression, decision tree
    "performance_baseline": 0.80,    # M√≠nimo aceptable
    "lift_required": 0.05,          # Champion debe superar por 5%+
    "complexity_justified": True     # Documentar por qu√© la complejidad vale la pena
}

# Evaluaci√≥n comparativa
def validate_champion_challenger(champion_model, challenger_model):
    champion_perf = evaluate(champion_model)
    challenger_perf = evaluate(challenger_model)
    
    lift = champion_perf - challenger_perf
    
    if lift < challenger_requirements["lift_required"]:
        return "Complejidad no justificada - usar challenger"
    
    return "Champion aprobado con lift de {:.2%}".format(lift)
```

#### 2.2 Evaluaci√≥n de Datos

```mermaid
graph LR
    A[Datos de Entrada] --> B[Calidad]
    A --> C[Representatividad]
    A --> D[Temporalidad]
    A --> E[Sesgos]
    
    B --> F{Apto?}
    C --> F
    D --> F
    E --> F
    
    F -->|S√≠| G[Continuar]
    F -->|No| H[Rechazar]
    
    style F fill:#ff6b6b
    style G fill:#51cf66
    style H fill:#ff6b6b
```

### Fase 3: Validaci√≥n T√©cnica (Semanas 3-4)

#### 3.0 Validaci√≥n Espec√≠fica para GenAI/LLMs

Para modelos generativos y LLMs, se requieren pruebas adicionales:

```python
# Suite de validaci√≥n para GenAI
genai_validation_suite = {
    "hallucination_test": {
        "method": "factual_consistency_check",
        "threshold": 0.95,  # 95% de respuestas deben ser verificables
        "dataset": "knowledge_base_test_set"
    },
    
    "prompt_injection_test": {
        "method": "owasp_llm_top10_attacks",
        "pass_rate": 0.99,  # 99% de ataques deben ser bloqueados
        "test_prompts": load_attack_prompts()
    },
    
    "data_leakage_test": {
        "method": "membership_inference_attack",
        "max_leakage": 0.001,  # <0.1% de informaci√≥n sensible expuesta
        "pii_patterns": load_pii_regex_patterns()
    },
    
    "toxicity_test": {
        "method": "perspective_api_scoring",
        "max_toxicity": 0.05,  # <5% de respuestas t√≥xicas
        "dataset": "real_toxicity_prompts"
    },
    
    "rag_accuracy": {  # Si usa Retrieval Augmented Generation
        "retrieval_precision": 0.90,
        "source_attribution": True,
        "hallucination_with_rag": 0.02  # Max 2% con RAG activo
    }
}

# Ejecutar validaci√≥n
for test_name, test_config in genai_validation_suite.items():
    result = run_genai_test(model, test_config)
    assert result.pass_rate >= test_config.get('threshold', 0.95)
```

#### 3.1 Replicaci√≥n Independiente
```python
# Proceso de replicaci√≥n
def replicate_model_training():
    """
    1. Usar datos y c√≥digo provistos
    2. Re-entrenar modelo desde cero
    3. Comparar m√©tricas con reportadas
    4. Tolerancia: ¬±5% relativo al valor original
    """
    original_metrics = load_reported_metrics()
    replicated_model = train_from_scratch()
    replicated_metrics = evaluate(replicated_model)
    
    # Tolerancia relativa, no absoluta
    relative_diff = abs(original_metrics - replicated_metrics) / original_metrics
    
    # Para modelos no determin√≠sticos (Deep Learning), permitir justificaci√≥n
    if relative_diff > 0.05:
        if model_type in ['deep_learning', 'ensemble'] and relative_diff < 0.10:
            return validation_with_justification
        else:
            return validation_failed
    
    return validation_passed
```

#### 3.2 Pruebas de Performance

| M√©trica | Tier 1 | Tier 2 | Tier 3 | Tier 4 |
|---------|--------|--------|--------|--------|
| **Accuracy/AUC** | >0.90 | >0.85 | >0.80 | >0.75 |
| **Precision** | >0.85 | >0.80 | >0.75 | >0.70 |
| **Recall** | >0.85 | >0.80 | >0.75 | >0.70 |
| **F1-Score** | >0.85 | >0.80 | >0.75 | >0.70 |
| **Estabilidad (PSI)** | <0.1 | <0.15 | <0.20 | <0.25 |

#### 3.3 An√°lisis de Robustez

##### Pruebas Adversariales
```python
# Test de robustez adversarial
adversarial_tests = {
    "noise_injection": add_gaussian_noise(data, sigma=0.1),
    "missing_features": randomly_drop_features(data, p=0.2),
    "distribution_shift": apply_covariate_shift(data),
    "adversarial_examples": generate_adversarial_samples(model)
}

for test_name, test_data in adversarial_tests.items():
    performance_drop = evaluate_model(model, test_data)
    assert performance_drop < MAX_ACCEPTABLE_DROP[model_tier]
```

##### An√°lisis de Sensibilidad
- Perturbaci√≥n de inputs ¬±10%
- Cambios en hiperpar√°metros
- Variaci√≥n en datos de entrenamiento
- Bootstrap sampling

### Fase 4: Validaci√≥n de Equidad (Semana 5)

#### 4.1 M√©tricas de Fairness

```python
# Evaluaci√≥n de equidad algor√≠tmica
fairness_metrics = {
    "demographic_parity": calculate_demographic_parity(predictions, protected_attr),
    "equal_opportunity": calculate_equal_opportunity(predictions, labels, protected_attr),
    "equalized_odds": calculate_equalized_odds(predictions, labels, protected_attr),
    "calibration": calculate_calibration(predictions, labels, protected_attr),
    "individual_fairness": calculate_individual_fairness(model, similar_individuals)
}

# Los umbrales se definen en la Evaluaci√≥n de Impacto √âtico (EIA) del modelo
# basados en el contexto espec√≠fico y requerimientos legales
fairness_targets = load_fairness_targets_from_eia(model_id)

# Validar contra targets espec√≠ficos del modelo
for metric_name, metric_value in fairness_metrics.items():
    target = fairness_targets.get(metric_name)
    
    if metric_value < target:
        risk_assessment = assess_residual_risk(metric_name, metric_value, target)
        
        if risk_assessment == "unacceptable":
            return validation_failed
        elif risk_assessment == "mitigable":
            return validation_conditional
        else:
            document_accepted_risk(metric_name, metric_value, target)

# Referencia: 80% rule para demographic parity es est√°ndar legal en EEUU
# pero puede variar seg√∫n jurisdicci√≥n y caso de uso
```

#### 4.2 An√°lisis de Sesgo

| Tipo de Sesgo | M√©todo de Detecci√≥n | Mitigaci√≥n Requerida |
|---------------|-------------------|---------------------|
| **Hist√≥rico** | An√°lisis temporal de datos | Re-balanceo o re-peso |
| **Representaci√≥n** | Distribuci√≥n demogr√°fica | Aumentaci√≥n de datos |
| **Medici√≥n** | Calidad diferencial por grupo | Correcci√≥n de medici√≥n |
| **Agregaci√≥n** | Performance por subgrupo | Modelos espec√≠ficos |
| **Evaluaci√≥n** | M√©tricas por segmento | M√©tricas balanceadas |

### Fase 5: Validaci√≥n de Explicabilidad (Semana 6)

#### 5.1 M√©todos de Explicaci√≥n

```mermaid
graph TD
    A[Modelo] --> B{Tipo}
    B -->|Linear/Tree| C[Interpretable]
    B -->|Deep Learning| D[SHAP/LIME]
    B -->|Ensemble| E[Feature Importance]
    B -->|GenAI/LLM| F[Attention Maps]
    
    C --> G[Documentar]
    D --> G
    E --> G
    F --> G
    
    G --> H[Validar Coherencia]
    H --> I[Aprobar]
    
    style A fill:#667eea
    style I fill:#51cf66
```

#### 5.2 Requisitos de Explicabilidad por Tier

| Tier | Requisito | M√©todo | Documentaci√≥n |
|------|-----------|---------|---------------|
| **1** | Explicaci√≥n individual completa | SHAP + Counterfactuals | Por cada decisi√≥n |
| **2** | Explicaci√≥n por segmento | LIME + Feature Importance | Por grupo de usuarios |
| **3** | Explicaci√≥n global | Partial Dependence Plots | Modelo completo |
| **4** | Explicaci√≥n b√°sica | Feature Importance | Top 10 features |

### Fase 6: Pruebas de Integraci√≥n (Semana 7)

#### 6.1 Testing en Ambiente de Pre-Producci√≥n

```python
# Suite de pruebas de integraci√≥n
integration_tests = {
    "latency": measure_inference_time() < SLA_LATENCY,
    "throughput": measure_requests_per_second() > MIN_THROUGHPUT,
    "memory": measure_memory_usage() < MAX_MEMORY,
    "concurrency": test_concurrent_requests(n=1000),
    "failover": test_fallback_mechanism(),
    "monitoring": verify_metrics_collection(),
    "logging": verify_audit_trail()
}

assert all(integration_tests.values()), "Integration tests failed"
```

#### 6.2 Validaci√≥n de APIs y Endpoints

| Prueba | Descripci√≥n | Criterio de √âxito |
|--------|-------------|------------------|
| **Contract Testing** | Validar schema de entrada/salida | 100% compliance |
| **Error Handling** | Manejo de inputs inv√°lidos | Graceful degradation |
| **Rate Limiting** | Protecci√≥n contra abuso | L√≠mites enforced |
| **Authentication** | Seguridad de endpoints | OAuth2/JWT v√°lido |
| **Versioning** | Compatibilidad backward | No breaking changes |

### Fase 7: Documentaci√≥n y Certificaci√≥n (Semana 8)

#### 7.1 Informe de Validaci√≥n

##### Estructura del Informe
1. **Resumen Ejecutivo**
   - Veredicto: Aprobado/Rechazado/Condicional
   - Principales hallazgos
   - Riesgos identificados
   - Recomendaciones

2. **Validaci√≥n Conceptual**
   - Idoneidad metodol√≥gica
   - Calidad de datos
   - Supuestos y limitaciones

3. **Validaci√≥n T√©cnica**
   - Performance metrics
   - Robustez y estabilidad
   - Escalabilidad

4. **Validaci√≥n de Equidad**
   - M√©tricas de fairness
   - An√°lisis de sesgo
   - Impacto diferencial

5. **Condiciones y Restricciones**
   - Casos de uso aprobados
   - Limitaciones operativas
   - Monitoreo requerido

#### 7.2 Certificaci√≥n

```mermaid
graph LR
    A[Informe Completo] --> B{Decisi√≥n}
    B -->|Aprobado| C[Certificado Verde]
    B -->|Condicional| D[Certificado Amarillo]
    B -->|Rechazado| E[No Certificado]
    
    C --> F[Deploy Autorizado]
    D --> G[Deploy con Restricciones]
    E --> H[Regreso a Desarrollo]
    
    style C fill:#51cf66
    style D fill:#ffd43b
    style E fill:#ff6b6b
```

## üîÑ Validaci√≥n Continua Post-Producci√≥n

### Monitoreo de Performance

```python
# KPIs de monitoreo continuo
monitoring_kpis = {
    "accuracy_drift": track_metric_over_time("accuracy", window="7d"),
    "prediction_drift": calculate_psi(current_dist, training_dist),
    "feature_drift": detect_covariate_shift(current_features, training_features),
    "fairness_drift": track_fairness_metrics(protected_attributes),
    "business_kpi": track_business_metrics(revenue_impact, customer_satisfaction)
}

# Umbrales de alerta
alert_thresholds = {
    "accuracy_drop": 0.05,  # 5% drop triggers alert
    "psi": 0.2,             # PSI > 0.2 indicates significant shift
    "fairness_violation": 0.1  # 10% degradation in fairness
}
```

### Calendario de Re-Validaci√≥n

| Tier | Validaci√≥n Completa | Validaci√≥n Parcial | Monitoreo |
|------|-------------------|-------------------|-----------|
| **1** | Semestral | Trimestral | Diario |
| **2** | Anual | Semestral | Semanal |
| **3** | Anual | Anual | Mensual |
| **4** | Bienal | Anual | Trimestral |

## üö® Proceso de Escalamiento

### Triggers de Re-Validaci√≥n Inmediata

1. **Cambio Material en Datos**
   - Nueva fuente de datos
   - Cambio en distribuci√≥n >20%
   - P√©rdida de features cr√≠ticas

2. **Degradaci√≥n de Performance**
   - Ca√≠da >5% en m√©trica principal
   - Incremento en quejas de clientes
   - Detecci√≥n de sesgos emergentes

3. **Cambio Regulatorio**
   - Nueva circular CNBV/Banxico
   - Actualizaci√≥n de normativa
   - Requerimiento de auditor√≠a

4. **Incidente de Producci√≥n**
   - Falla del modelo
   - Decisiones incorrectas sistem√°ticas
   - Brecha de seguridad

## üìã Checklist de Validaci√≥n R√°pida

### Para Product Owners
- [ ] ¬øDocumentaci√≥n completa entregada?
- [ ] ¬øC√≥digo versionado en Git?
- [ ] ¬øDatos de prueba disponibles?
- [ ] ¬øM√©tricas de baseline definidas?
- [ ] ¬øStakeholders notificados?

### Para Validadores
- [ ] ¬øModelo replicable?
- [ ] ¬øPerformance aceptable?
- [ ] ¬øFairness verificada?
- [ ] ¬øRobustez probada?
- [ ] ¬øDocumentaci√≥n clara?

### Para Risk Management
- [ ] ¬øRiesgos identificados?
- [ ] ¬øControles implementados?
- [ ] ¬øCumplimiento regulatorio?
- [ ] ¬øPlan de contingencia?
- [ ] ¬øMonitoreo definido?

## üõ†Ô∏è Herramientas y Recursos

### Stack de Validaci√≥n

```yaml
# Herramientas aprobadas para validaci√≥n
validation_tools:
  statistical:
    - Python: scipy, statsmodels
    - R: forecast, tidymodels
  
  fairness:
    - Fairlearn
    - AIF360 (AI Fairness 360)
    - What-If Tool
  
  explainability:
    - SHAP
    - LIME
    - InterpretML
  
  adversarial:
    - Adversarial Robustness Toolbox
    - Foolbox
    - CleverHans
  
  monitoring:
    - Evidently AI
    - WhyLabs
    - Arize AI
```

### Plantillas y Formatos

| Documento | Plantilla | Ubicaci√≥n |
|-----------|-----------|-----------|
| Model Development Document | MDD_Template_v2.docx | /templates/validation/ |
| Validation Report | VAL_Report_Template.docx | /templates/validation/ |
| Risk Assessment | RISK_Template.xlsx | /templates/risk/ |
| Fairness Analysis | FAIR_Template.ipynb | /notebooks/validation/ |
| Monitoring Dashboard | MON_Dashboard.json | /dashboards/ |

## üìä M√©tricas de √âxito del Proceso

### KPIs de Validaci√≥n

```python
# M√©tricas del proceso de validaci√≥n
validation_process_metrics = {
    "cycle_time": {
        "target": "8 weeks",
        "current": calculate_average_cycle_time()
    },
    "first_pass_rate": {
        "target": 0.70,
        "current": approved_first_time / total_validations
    },
    "defect_escape_rate": {
        "target": 0.02,
        "current": production_issues / deployed_models
    },
    "compliance_rate": {
        "target": 1.00,
        "current": compliant_models / total_models
    }
}
```

### Dashboard de Validaci√≥n

| M√©trica | Q1 2025 | Q2 2025 | Target | Status |
|---------|---------|---------|--------|--------|
| Modelos Validados | 12 | 18 | 15 | ‚úÖ |
| Tiempo Promedio | 7.5 sem | 6.8 sem | 8 sem | ‚úÖ |
| Tasa de Aprobaci√≥n | 75% | 82% | 70% | ‚úÖ |
| Incidentes Post-Deploy | 1 | 0 | <2 | ‚úÖ |

## üìö Referencias y Normativa

### Documentos Regulatorios
- CNBV Circular √önica de Bancos - Anexo 33 (Riesgo Operacional)
- Banxico Circular 4/2019 - Validaci√≥n de Modelos
- Basel Committee BCBS 239 - Risk Data Aggregation
- SR 11-7 Federal Reserve - Model Risk Management

### Est√°ndares Internacionales
- ISO/IEC 23053:2022 - AI Trustworthiness
- ISO/IEC 23894:2023 - AI Risk Management
- IEEE 7000-2021 - Model Process for Addressing Ethical Concerns

### Mejores Pr√°cticas
- Google Model Cards for Model Reporting
- Microsoft Responsible AI Standard v2
- IBM AI Fairness 360 Toolkit Documentation

## ü§ù Contacto y Soporte

### Equipo de Validaci√≥n de Modelos
- **Email**: model-validation@novasolutionsystems.com
- **Teams**: #model-risk-management
- **Horario**: Lun-Vie 8:00-18:00
- **Escalamiento**: mrm-escalation@novasolutionsystems.com

### Recursos Adicionales
- [Pol√≠tica de Uso Responsable de IA](politica-uso-responsable-ia.md)
- [Framework AISIA](framework-aisia.md)
- [Gu√≠a de MLOps](../recursos/documentacion-tecnica/mlops-guide.md)
- [Nova-Cell Platform](../servicios/plataforma/nova-cell-hub.md)

---

**Versi√≥n**: 1.0  
**Fecha de emisi√≥n**: Enero 2025  
**Pr√≥xima revisi√≥n**: Julio 2025  
**Clasificaci√≥n**: CONFIDENCIAL - USO INTERNO